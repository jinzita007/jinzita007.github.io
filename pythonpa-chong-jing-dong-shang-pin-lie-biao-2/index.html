

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <link href="../assets/css/main.css?v=5e892d17e0" rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css" href="../assets/css/screen.css?v=5e892d17e0">
    <link rel="stylesheet" href="https://highlightjs.org/static/demo/styles/github.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.15.6/highlight.min.js"></script>
    

    <title></title>
     <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon">
    <link rel="canonical" href="index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="amphtml" href="amp/index.html">
    
    <meta property="og:site_name" content="无根树の博客">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Python爬虫京东商品列表">
    <meta property="og:description" content="当我开发一个后台Node的时候，插入数据库多麻烦了，想到的是可以利用Python爬虫商品列表存到MySQL数据库，可以省去了插入数据库的麻烦。 遍历京东商品列表代码如下： from urllib.request import urlopen from bs4 import BeautifulSoup  #导入库 url = &amp;quot;http://search.yhd.com/c0-0-1003817/mbname-b/a-s1-v4-p1-price-d0-f0-m1-rt0-pid-mid0-color-size-k/?ref=ad.21102_47515303_1&amp;quot;;  html = urlopen(url)  #请求网址 soup = BeautifulSoup(html, &amp;quot;lxml&amp;quot;)  #解析网页信息 proPrice = soup.select(&amp;quot;p.proPrice em&amp;quot;)  #抓取价格">
    <meta property="og:url" content="http://localhost:2368/pythonpa-chong-jing-dong-shang-pin-lie-biao-2/">
    <meta property="article:published_time" content="2018-02-04T04:45:02.000Z">
    <meta property="article:modified_time" content="2018-02-04T04:36:51.000Z">
    <meta property="article:tag" content="python">
    <meta property="article:tag" content="beautifulsoup">
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Python爬虫京东商品列表">
    <meta name="twitter:description" content="当我开发一个后台Node的时候，插入数据库多麻烦了，想到的是可以利用Python爬虫商品列表存到MySQL数据库，可以省去了插入数据库的麻烦。">
    <meta name="twitter:url" content="http://localhost:2368/pythonpa-chong-jing-dong-shang-pin-lie-biao-2/">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="python, beautifulsoup">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "无根树の博客",
        "logo": {
            "@type": "ImageObject",
            "url": "http://localhost:2368/favicon.ico",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "",
        "url": "http://localhost:2368/author/",
        "sameAs": []
    },
    "headline": "Python爬虫京东商品列表",
    "url": "http://localhost:2368/pythonpa-chong-jing-dong-shang-pin-lie-biao-2/",
    "datePublished": "2018-02-04T04:45:02.000Z",
    "dateModified": "2018-02-04T04:36:51.000Z",
    "keywords": "python, beautifulsoup",
    "description": "当我开发一个后台Node的时候，插入数据库多麻烦了，想到的是可以利用Python爬虫商品列表存到MySQL数据库，可以省去了插入数据库的麻烦。 遍历京东商品列表代码如下： from urllib.request import urlopen from bs4 import BeautifulSoup  #导入库 url &#x3D; &amp;quot;http://search.yhd.com/c0-0-1003817/mbname-b/a-s1-v4-p1-price-d0-f0-m1-rt0-pid-mid0-color-size-k/?ref&#x3D;ad.21102_47515303_1&amp;quot;;  html &#x3D; urlopen(url)  #请求网址 soup &#x3D; BeautifulSoup(html, &amp;quot;lxml&amp;quot;)  #解析网页信息 proPrice &#x3D; soup.select(&amp;quot;p.proPrice em&amp;quot;)  #抓取价格",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <script type="text/javascript" src="../public/ghost-sdk.min.js?v=5e892d17e0"></script>
<script type="text/javascript">
ghost.init({
	clientId: "ghost-frontend",
	clientSecret: "a41d73990106"
});
</script>
    <meta name="generator" content="Ghost 1.20">
    <link rel="alternate" type="application/rss+xml" title="无根树の博客" href="../rss/index.html">
</head>

<body class="post-template tag-python tag-beautifulsoup">

    
 <div class="header">
    <div class="header-inder">
        <nav class="site-navs">
            <div class="site-nav-left">
                <a class="site-nav-logo" href="../">无根树の博客</a>
                  <ul class="nav" role="menu">
    <li class="nav-" role="menuitem"><a href="../">首页</a></li>
    <li class="nav-" role="menuitem"><a href="../records/">归档</a></li>
    <li class="nav-" role="menuitem"><a href="../tags/">标签</a></li>
    <li class="nav-github" role="menuitem"><a href="https://github.com/jinzita007">github</a></li>
</ul>
 
            </div>
            <div class="site-nav-right">
                <form class="form-inline my-2 my-lg-0">
                    <input id="search-field" class="form-control mr-sm-2" type="text" placeholder="Search">
                    <!--<button class="btn btn-secondary my-2 my-sm-0" type="submit">搜索</button>-->
                </form>
            </div>

        </nav>
    </div>
</div>
<section id="results-main">
    <ul id="results" class="results-search">
    </ul>
</section>
    <div class="container">
        <div class="wrap">
            <div class="content-main">
                 
                <div class="content-title">
                    <h1>Python爬虫京东商品列表</h1>
                    <span class="post-main">
                        <span class="post-time">
                            <span class="post-meta-item-icon">
                                <i class="fa fa-calendar-o"></i>
                            </span>
                            <time class="post-meta-time">2018-02-04</time>
                        </span>  |   
                        <span class="tags">
                             <a href="../tag/python/">python</a> <a href="../tag/beautifulsoup/">beautifulsoup</a>
                        </span>
                    </span>
                </div>
                <div class="content-dec">
                    
                    <div class="kg-card-markdown"><blockquote>
<p>当我开发一个后台Node的时候，插入数据库多麻烦了，想到的是可以利用Python爬虫商品列表存到MySQL数据库，可以省去了插入数据库的麻烦。</p>
</blockquote>
<blockquote>
<p>遍历京东商品列表代码如下：</p>
</blockquote>
<pre><code>from urllib.request import urlopen
from bs4 import BeautifulSoup  #导入库

url = "http://search.yhd.com/c0-0-1003817/mbname-b/a-s1-v4-p1-price-d0-f0-m1-rt0-pid-mid0-color-size-k/?ref=ad.21102_47515303_1"; 
html = urlopen(url)  #请求网址
soup = BeautifulSoup(html, "lxml")  #解析网页信息

proPrice = soup.select("p.proPrice em")  #抓取价格
pdlink2 = soup.select('p.proName &gt; a')   #抓取商品名称

for title,price in zip(pdlink2,proPrice):  
    title = title.contents[-1]     #将列表中的每一个商品信息提取出来
    price = price.contents[-1]     #将列表中的每一个商品价格提取出来
    print("[*] 商品标题:", title)   #输出商品标题
    print("[*] 商品价格:", price)   #输出商品价格
</code></pre>
<blockquote>
<p>注意的是：用一个zip()函数实现一次遍历</p>
</blockquote>
<blockquote>
<p>输出商品列表：</p>
</blockquote>
<pre><code>[*] 商品标题: 
印尼进口 Danisa 皇冠 丹麦 曲奇 454g（新旧包装随机发货） 盒装

[*] 商品价格: 49.90

[*] 商品标题: 
澳大利亚 进口牛奶 德运（Devondale） 全脂牛奶 1L*10 整箱装
......
</code></pre>
<blockquote>
<p>建一个列表</p>
</blockquote>
<pre><code>data = [] # 建一个列表，用于存放数据
</code></pre>
<blockquote>
<p>填入列表</p>
</blockquote>
<pre><code>data.append([title,price]) ##将爬取的数据依次填入列表中
print(data) ##输出
</code></pre>
<blockquote>
<p>打印出结果</p>
</blockquote>
<pre><code>[['\n印尼进口 Danisa 皇冠 丹麦 曲奇 454g（新旧包装随机发货） 盒装\n', '49.90\n']]
[['\n澳大利亚 进口牛奶 德运（Devondale） 全脂牛奶 1L*10 整箱装\n', '99.00\n']]
</code></pre>
<blockquote>
<p>但是需要移除<code>\n</code>,可以通过正则表达式和替换方法</p>
</blockquote>
<pre><code># 正则表达式
    dr = re.compile(r'\n|',re.S)
    # 正则的替换
    dd_title = dr.sub('', title)
    dd_price = dr.sub('', price)
    # 移除空格
    dd_s_title = dd_title.strip()
    dd_s_price = dd_price.strip()
</code></pre>
<blockquote>
<p>要修改填入列表</p>
</blockquote>
<pre><code>data.append([dd_s_title,dd_s_price]) ##将爬取的数据依次填入列表中
print(data) ##输出
</code></pre>
<blockquote>
<p>打印输入查看</p>
</blockquote>
<pre><code>[['印尼进口 Danisa 皇冠 丹麦 曲奇 454g（新旧包装随机发货） 盒装', '49.90']]
[['澳大利亚 进口牛奶 德运（Devondale） 全脂牛奶 1L*10 整箱装', '99.00']]
</code></pre>
<blockquote>
<p>这样正常OK了</p>
</blockquote>
<blockquote>
<p>要增加mysql导入库</p>
</blockquote>
<pre><code>import mysql.connector
</code></pre>
<blockquote>
<p>连接mysql数据库，要先设置好用户、密码和数据库名称</p>
</blockquote>
<pre><code>#连接mysql数据库
conn = mysql.connector.connect(user='root', password='root', database='mynode')
#使用cursor()方法获取操作游标
cur = conn.cursor()
</code></pre>
<blockquote>
<p>然后抓取内容批量插入到goods表<br>
向goods表(title,price)插入数据</p>
</blockquote>
<pre><code>sql = "INSERT INTO goods (title,price) values(%s,%s)"  # 这是一条sql插入语句
</code></pre>
<blockquote>
<p>这样就包含了两条数据，通过executemany插入</p>
</blockquote>
<pre><code class="language-python">cur.executemany(sql, data)  # 执行sql语句，并用executemany()函数批量插入数据库中
conn.commit()
</code></pre>
<blockquote>
<p>注意的是如果没有python异常处理，可能无法存到数据库。要加上try...except.</p>
</blockquote>
<pre><code>    try:
        title = title.contents[-1]   #将列表中的每一个商品信息提取出来
        price = price.contents[-1]   #将列表中的每一个商品价格提取出来
        ...
        conn.commit()
    except Exception as e:
        print(e)
        conn.rollback()
</code></pre>
<blockquote>
<p>插入OK就关闭数据库</p>
</blockquote>
<pre><code># 释放数据连接
if cur:
    cur.close()
if conn:
    conn.close()
</code></pre>
<blockquote>
<p>完整项目</p>
</blockquote>
<pre><code># -*- coding: utf-8 -*-
#---------------------------------------
#   程序：商品爬虫
#   版本：0.0.2
#   作者：woaitianwen
#   日期：2018-2-3
#   语言：Python 3.6
#   操作：输入网址后就获取商品列表，然后存到mysql数据库
#---------------------------------------


from urllib.request import urlopen
from bs4 import BeautifulSoup
import mysql.connector

#连接mysql数据库
conn = mysql.connector.connect(user='root', password='root', database='mynode')
#使用cursor()方法获取操作游标
cur = conn.cursor()

url = "http://search.yhd.com/c0-0-1003817/mbname-b/a-s1-v4-p1-price-d0-f0-m1-rt0-pid-mid0-color-size-k/?ref=ad.21102_47515303_1";
html = urlopen(url)  #请求网址
soup = BeautifulSoup(html, "lxml")  #解析网页信息

proPrice = soup.select("p.proPrice em")  #抓取价格
pdlink2 = soup.select('p.proName &gt; a')   #抓取商品名称

for title,price in zip(pdlink2,proPrice):
    data = [] # 建一个列表，用于存放数据
    try:
        title = title.contents[-1]     #将列表中的每一个商品信息提取出来
        price = price.contents[-1]     #将列表中的每一个商品价格提取出来
        # 正则表达式
        dr = re.compile(r'\n|',re.S)
        # 正则的替换
        dd_title = dr.sub('', title)
        dd_price = dr.sub('', price)
        # 移除空格
        dd_s_title = dd_title.strip()
        dd_s_price = dd_price.strip()

        #print("[*] 商品标题:", title)  #输出商品标题
        #print("[*] 商品价格:", price)  #输出商品价格

        data.append([dd_s_title,dd_s_price]) ##将爬取的数据依次填入列表中

        print(data)
        sql = "INSERT INTO goods (title,price) values(%s,%s)"  # 这是一条sql插入语句
        cur.executemany(sql, data)  # 执行sql语句，并用executemany()函数批量插入数据库中
        conn.commit()
    except Exception as e:
        print(e)
        conn.rollback()


# 释放数据连接
if cur:
    cur.close()
if conn:
    conn.close()
</code></pre>
<blockquote>
<p>插入成功数据库效果</p>
</blockquote>
<p><img src="../content/images/2018/02/Snip20180204_20.png" alt="Snip20180204_20"></p>
</div>
                </div>

            </div>
        </div>
    </div>

    <script>
   
    </script> 


</body>
<script src="https://cdn.bootcss.com/jquery/2.2.0/jquery.min.js"></script>
<script src="../assets/js/jquery.ghostHunter.js?v=5e892d17e0" type="text/javascript"></script>
<script>

    //实时监听input框输入值并增加搜索
    var input = document.getElementById('search-field');
    $("#search-field").ghostHunter({
        results: "#results",
        onKeyUp: true,
        displaySearchInfo: false,
        before: function () {
            input.oninput = function () {
                //如果输入值为空时，列表内容就显示出来
                if (input.value === '') {
                    //console.log(input.value);
                    $(".wrap-active").removeClass("results-hide");
                } else {
                    //如果输入值为有字符，列表内容就隐藏
                    //console.log(input.value);
                    $(".wrap-active").addClass("results-hide");
                }
            }
        }
    });

</script>
<script>hljs.initHighlightingOnLoad();</script>


